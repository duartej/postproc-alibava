#!/usr/bin/env python
"""Multitool for the processing of the ALiBaVa and Telescope data.
The available sub-commands are:
    * list_files: Extract the list of alibava raw data files from a parent 
      folder. The folder should contain subdirectories with the name of the
      involved sensors, and the file names follows a pre-defined naming 
      convention.
    
    * steering: Build the needed steering files to run a given step of the 
      marlin framework reconstruction for the ALIBAVA or TELESCOPE data. 
      Note that the steering file is created in the working directory. All
      the available options but the `--print-availble-steps` are directly 
      related with the relevant steering file (see the steering files under
      `steering_files` directory). Two meta-steps which prepare the full
      sequence needed are available for the AliBaVa data and the Telescope
      data. 

   * sensor_maps: Produce charge and efficiency maps for the ALiBaVa data 
     acquired sensors, processed using the Marlin framework. The ALiBaVa 
     data has been merged with the Telescope data (EUTelescope software) 
     and has provided a ROOT NTUPLE file with a specific structure 
     containing both the ALiBaVa sensors data and reconstructed tracks from
     the telescope (this is the NTUPLE input for script).

   * merge_eore: Create the steering file to be used to merge a collection
     of slcio files containing the 'End of Run Event' (EORE). The usual 
     method (lcio_merge_files) does not take into account that last event 
     added by the EUTelescope processors and it will result in a lost of 
     synchronization. Using the created steering file here, the EOREs will
     be removed and the events merged properly.
"""
__author__ = "Jordi Duarte-Campderros"
__credits__ = ["Jordi Duarte-Campderros"]
__version__ = "v0.1"
__maintainer__ = "Jordi Duarte-Campderros"
__email__ = "jorge.duarte.campderros@cern.ch"
__status__ = "Development"

def list_raw_files(parent_folder,verbose,open_sesame_format,beam_file=None):
    """Extract the list of alibava raw data files from a parent folder.
    The folder should contain subdirectories with the name
    of the involved sensors, and the file names follows a pre-defined
    naming convention.

    If `beam_file` is not None, the function will print the pedestal
    and calibration files (absolute paths) associated to that beam
    file; raising a RuntimeError if no pedestal and/or calibration
    file have been found, or if more than one pedestal and/or calibration
    file have been found.

    Parameters
    ----------
    parent_folder: str
        the path under where the sensors subdirectories must be localized
    verbose: bool
        whether or not to dump out more information whenever a file has
        been not included
    open_sesame_format: bool
        whether or not the output will be sent with the expected options
        needed to call the 'open_sesame steering alibava_full_reco' command
    beam_file: str | None
        if is not None, the name (basename) of a beam-type alibava RAW
        file
    """
    import alibavaSkifftools.SPS2017TB_metadata as tb2017
    import glob
    import os

    # -- Check the sensors paths are there
    sensor_paths = [ os.path.join(parent_folder,s) for s in tb2017.sensor_names ]
    if False in filter(lambda abspath: os.path.isdir(abspath), sensor_paths):
        raise IOError("Invalid parent folder structure at '{0:s}', "\
            "missing sensors direcotories ".format(parent_folder))
    # -- 
    ###if sensor not in tb2017.sensor_names:
    ###    raise RuntimeError("Not a valid sensor name '{0}'".format(sensor))
    # -- Descend over each directory and fill a dictionary with 
    #    filename_parser instances
    sensor_files = {}
    failed = []
    for sensor in tb2017.sensor_names:
        sensor_files[sensor] = []
        beam_files =[]
        ped_cal = []
        # Find all the raw alibava data files
        for fname in glob.glob(os.path.join(os.path.join(parent_folder,sensor),"*.dat")):
            try:
                _prv = tb2017.filename_parser(fname)
            except RuntimeError:
                # The file-name format is not understood
                if fname.find("RunN") == -1 and not open_sesame_format:
                    print "Just ignoring '{0}\n'".format(fname) 
                continue
            if _prv.is_beam:
                beam_files.append(_prv)
            else:
                ped_cal.append(_prv)
        # After split between beam and auxiliary runs, associate them
        for _fb in beam_files:
            try:
                sensor_files[sensor].append(tb2017.associated_filenames(_fb,ped_cal))
            except IOError:
                # No pedestals nor calibration, skip this file
                # Store if verbose
                if verbose:
                    failed.append( (_fb,'NO CAL/PED FOUND') )
                continue
            except RuntimeError as e:
                # One pedestal or one calibration
                # Store if verbose
                if verbose:
                    failed.append( (_fb,str(e)) )
                continue
        # If a beam file is provided, check if the file is in this sensor folder
        if beam_file is not None:
            if os.path.basename(beam_file) in \
                    map(lambda x: os.path.basename(x.beam_instance.filename),sensor_files[sensor]):
                tf_inst = filter(lambda x: os.path.basename(beam_file) in \
                        os.path.basename(x.beam_instance.filename),\
                        sensor_files[sensor])[0]
                print "{0},{1}\n".format(tf_inst.pedestal_instance.filename,tf_inst.calibration_instance.filename)
                return
            continue
    if beam_file is not None:
        # Not found, so a raise the error
        raise RuntimeError("Not found '{0}'".format(beam_file))

    for sname,flist in sensor_files.iteritems():
        if not open_sesame_format:
            print "\033[1;34mSENSOR:\033[1;m \033[1;20m{0}\033[1;m".format(sname)
        for af in flist:
            if open_sesame_format:
                print '{0}@{1}@{2}@{3}@{4}'.format(af.beam_instance.run_number,\
                        tb2017.get_standard_sensor_name(af.beam_instance.sensor_name),\
                        af.beam_instance.filename,af.pedestal_instance.filename,af.calibration_instance.filename)
            else:
                print af
    if verbose:
        print "\033[1;33mVERBOSE: Sensors and runs with incomplete/inconsistent files\033[1;m"
        for (finst,pedcallist) in failed:
            print "{0}, run: {1}, '{2}'".format(finst.sensor_name,finst.run_number,pedcallist)

def create_steering_file(step_name,**args):
    """Create the steering file for the given step. The steering
    file will be created in the working directory. If no extra
    arguments are provided, the default will be applied

    Parameters
    ----------
    step_name: str
        the name of the step. Must correspond to a concrete class 
        of alibavaSkifftools.steering_processing.marlin_step

    ALIBAVA_INPUT_FILENAME: str, optional
        The input file name of the RAW alibava file

    See also
    --------
    alibavaSkifftools.steering_processing.marlin_step and
    their concrete classes, to get a list of valid and needed 
    template arguments
    """
    from alibavaSkifftools import steering_processing

    # create the marlin step instance
    step = steering_processing.create_marlin_step(step_name)
    # And create the steering file
    step.publish_steering_file(**args)

def sensor_map_production(ntuplename):
    """Produce the charge maps and the efficiency maps (hit 
    reconstruction efficiency) for the sensor included in the 
    NTUPLE. Wrapper to the method at alibavaSkifftools.marlin_postprocessor
    See details in there

    Parameters
    ----------
    ntuplename: str
        The name of the ROOT ntuple. The filename MUST follow
        the standard notation defined through the class 
        alibavaSkifftools.SPS2017TB_metadata.filename_parser 
    """
    from alibavaSkifftools.marlin_postprocessor import sensor_map_production
    
    # Just do the job
    sensor_map_production(ntuplename)


def merge_files(lcio_files,output_name):
    """Merge lcio files containing EORE in order to remove those 
    events. The algorithm will pick every found set of numbers, and
    will use them, previously discarding those sets common to all the names,
    to ordered based on that. 

    Parameters
    ----------
    lcio_files: list(str)
        the list of LCIO files with EORE to be merged
    output_name: str
        the name of the merged file [Default:

    Returns
    -------
    steering_filename: str
        The name of the steering file
    """
    import os
    import re
    # Mini class to deal with the sorting problem
    # A list of integers extracted from the filenames
    # are used as the handles to order the files. Assumes
    # that all the integers in the list are the same but one
    class mini_list(object):
        def __init__(self,intlist):
            """List of integers
            """
            self.intlist = intlist
        def __eq__(self,other):
            for i,val in enumerate(self.intlist):
                if val != other.intlist[i]:
                    return False
            return True

        def __lt__(self,other):
            if self == other:
                return False
            for i,val in enumerate(self.intlist):
                if val == other.intlist[i]:
                    continue
                if val > other.intlist[i]:
                    return False
            return True
        
        def __leq__(self,other):
            if self == other:
                return True
            if self < other:
                return True
            return False

    # Map of the tuples of set of numbers against their name
    discriminant_map = {}
    for fname in lcio_files:
        # Extract the subset of numbers
        discriminant_map[fname] = mini_list(map(lambda n: int(n),filter(None,re.findall(r'\d*',fname))))
    # And ordered using the integer list
    mergefiles_str =" ".join(map(lambda (k,v): k,sorted(discriminant_map.iteritems(),key=lambda (x,y): y)))

    steering_content = """<?xml version="1.0" encoding="us-ascii"?>
<!-- ?xml-stylesheet type="text/xsl" href="http://ilcsoft.desy.de/marlin/marlin.xsl"? -->
<!-- ?xml-stylesheet type="text/xsl" href="marlin.xsl"? -->

<!--Automaticaly created to gather all output files from a cluster jobs-->

<marlin xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://ilcsoft.desy.de/marlin/marlin.xsd">
    <execute>
      <processor name="Save"/>
      <processor name="MyEUTelUtilityPrintEventNumber"/>
   </execute>
   
   <global>
      <parameter name="LCIOInputFiles">{0} </parameter>
      <!-- Not needed, just will show a warning-->
      <!--parameter name="GearXMLFile" value="dummy_gear.xml"/-->
      <parameter name="MaxRecordNumber" value="9999999"/>
      <parameter name="SkipNEvents" value="0"/>
      <parameter name="SupressCheck" value="false"/>
      <parameter name="Verbosity" value="WARNING"/>
   </global>

   <processor name="Save" type="EUTelOutputProcessor">
       <!--Writes the current event to the specified LCIO outputfile. 
            Eventually it adds a EORE at the of the file if it was missing Needs to be the last ActiveProcessor.-->
       <!-- name of output file -->
       <parameter name="LCIOOutputFile" type="string">{1} </parameter>
       <!--write mode for output file:  WRITE_APPEND or WRITE_NEW-->
       <parameter name="LCIOWriteMode" type="string" value="WRITE_NEW"/>
       <!--Set it to true to remove intermediate EORE in merged runs-->
       <parameter name="SkipIntermediateEORE" type="bool" value="true"/>
   </processor>
   
   <processor name="MyEUTelUtilityPrintEventNumber" type="EUTelUtilityPrintEventNumber">
       <!--EUTelUtilityPrintEventNumber prints event number to screen depending on the verbosity level-->
       <!--Print event number for every n-th event-->
       <parameter name="EveryNEvents" type="int" value="5000"/>
   </processor>
</marlin>
""".format(mergefiles_str,output_name)
    stname = "merge_files_steering.xml"
    with open(stname,"w") as f:
        f.write(steering_content)
    return stname

def sensor_analysis(args):
    """Post-processing of the ROOT files created by the 'fortythieves'
    executable, including cluster finding, position and sensor
    characteristics analysis (eta distributions, charge distributions, ...)

    Parameters
    ----------
    args: ArgumentParser.Namespace
        The passes arguments to the main script, with the arguments 
        (some of them are optional). The full list is described 
        below and accesible by args.<name_argument>
    root_file: str
        The name of the input ROOT generated by the `fortythieves`
        executable
    chip_selection: int, optional
        The chip to process. If it is not present it will use the 
        recommendation of alibavaSkifftools.SPS2017TB_metadata module 
    polarity: int
        The signal polarity. Only used if the file name does not follow
        the standard naming convention, which in that case, this argument
        is ignored
    time_window: (float,float) 
        The TDC time window to cut
    snr_seed: float
        The Signal to Noise cut to be considered a channel as a seed
        of a cluster
    snr_neighbour: float
        The Signal to Noise cut to be considered a channel into a cluster
    masked_channels: list(int), optional
        Manual masking channels. If not present indicates that the channel
        masking is done automaticaly (using the criteria defined at 
        `automask_criterium`)
    automask_criterium: float, optional
        The criterium to used to decide if a channel is noisy or not. A 
        noisy channel is defined as |Noise_i - <Noise>| > X sigma. 
        If `masked_channels` is present, this has no effect
    xtcorrection: int
        The number of iterations needed for the FIR filter when applying 
        the inductive cross-talk correction. A value of 0, no correction
        is applied
    """
    from alibavaSkifftools.SPS2017TB_metadata import filename_parser
    from alibavaSkifftools.SPS2017TB_metadata import standard_sensor_name_map
    from alibavaSkifftools.SPS2017TB_metadata import sensor_name_spec_map
    from alibavaSkifftools.SPS2017TB_metadata import get_beetle
    from alibavaSkifftools.alibava_analysis import alibava_analysis
    
    # -- Obtain user instructions to configure and call the analysis

    # Mandatory configurables: note that the dict-keys follow exactly the same
    # name of the configurables in the `alibava_analysis` class (see
    # alibava_analysis.configuration
    configurables = { "chip_selection": None, "time_window": args.time_window, \
            "snr_seed": args.snr_seed, "snr_neighbour": args.snr_neighbour, \
            "noautomask": args.noautomask, "automask_criterium": args.automask_criterium, \
            "xtcorrection": args.xtcorrection, "polarity": args.polarity} 
    # Check if the filename follows standard naming convections
    fnp = None
    try:
        fnp = filename_parser(args.root_file)
    except NameError:
        # Not following naming convention, user must provided
        # all needed configurables
        pass

    # Check if the user provides all the needed configurables
    if not args.chip_selection:
        if not fnp:
            raise RuntimeError("'--chip-selection' option must be "\
                    "defined as the filename does not follow the standard "
                    "naming convention (this info is parsed from the file name)")
        configurables["chip_selection"] = get_beetle(standard_sensor_name_map[fnp.sensor_name])
    # Masking channels automatically
    if not configurables["noautomask"]:
        if args.masked_channels != None:
            raise RuntimeError("The '--masked_channels' option (masking channels "\
                    "by the user) requires to activate '--no-automask' option")
            # Prepare to do it later: left it empty
        # configurables["masked_channels"] = []
    else:
        # masking following user instructions
        if not args.masked_channels: 
            raise RuntimeError("The '--no-automask' option requires "\
                    " the '--masked_channels' option (masking channels "\
                    "by the user)")
        configurables["masked_channels"] = args.masked_channels
    # Polarity obtained from the metadata (if it is posible)
    if fnp:
        configurables["polarity"] = int(sensor_name_spec_map[standard_sensor_name_map[fnp.sensor_name]].polarity)
    elif not configurable["polarity"]:
        raise RuntimeError("Given the non-standard name of '{0}', it is not possible "\
                "to automatically deduce the polarity of the sensor: use the '--polarity'"\
                " option to introduce it manually")

    ## -- Ready to go
    an_inst = alibava_analysis(args.root_file,configurables["chip_selection"])
    an_inst.configure(**configurables)
    ## -- initialize data and algorithms
    an_inst.initialize()
    ## Initialize persistifier
    an_inst.book_results(args.root_file.replace(".root","_analysis_cluster.root"))
    ## Process all events and store results
    an_inst.process()


if __name__ == '__main__':
    from argparse import ArgumentParser,Action
    from alibavaSkifftools.SPS2017TB_metadata import eospath,sensor_names
    from alibavaSkifftools.steering_processing import _ARGUMENTS as template_args

    # Helper class to allow list sensor names (without introducing
    # the positional arguments
    class SensorNamesAction(Action):
        def __init__(self,option_strings,dest,default=False,required=False,help=None):
            super(SensorNamesAction, self).__init__(
                    option_strings=option_strings,
                    dest=dest,
                    nargs=0,
                    const=True,
                    default=default,
                    required=required,
                    help=help)

        def __call__(self, parser, namespace, values, option_string=None):
            print "\033[1;34mAvailable sensor names:\033[1;m"
            for sensor in sensor_names:
                print " - {0}".format(sensor)
            parser.exit()
    
    # Helper class to allow list steering steps (without introducing
    # the positional arguments
    class StepNamesAction(Action):
        def __init__(self,option_strings,dest,default=False,required=False,help=None):
            super(StepNamesAction, self).__init__(
                    option_strings=option_strings,
                    dest=dest,
                    nargs=0,
                    const=True,
                    default=default,
                    required=required,
                    help=help)

        def __call__(self, parser, namespace, values, option_string=None):
            from alibavaSkifftools.steering_processing import available_steps as _steps
            print "\033[1;34mAvailable steps:\033[1;m"
            for step in _steps:
                print " - \033[1;29m{0}\033[1;m: {1}".format(step.__name__,step.get_description())
            parser.exit()
    
    mesdsc="A suite of tools to deal with the ALIBAVA (and EUTelescope)"
    mesdsc+=" data output. Get the description of each subcommand with"
    mesdsc+=" help option."
    parser = ArgumentParser(prog='open_sesame',description=mesdsc)
    
    # Sub-command parsers
    subparsers = parser.add_subparsers(title='subcommands',
            description='valid subcommands', 
            help='additional help')
    
    # -- Subparser: list data sensor, test beam data associated to their
    #               respective pedestal and calibration files 
    usage_ld  = "Extract the list of alibava raw data files from a parent folder. "
    usage_ld += "The folder should contain subdirectories with the name of the"
    usage_ld += " involved sensors; and the file names follow a pre-defined naming "
    usage_ld += "convention. If `--beam-filename` option is used, the pedestal,calibration"
    usage_ld += " files will be printed instead."
    
    ld_parser = subparsers.add_parser("list_files",description=usage_ld)
    ld_parser.add_argument('parent_folder',help="The parent folder to start"\
            " to search down [Default: see SPS2017TB_metadata.eospath]")
    ld_parser.add_argument('--formatted',dest='op_format',action='store_true',\
            help="Activating this flag, the output will be formatted as: "\
            "'RunNumber@SensorName@beam_rawfile@pedestal_rawfile@calibration_rawfile'"\
            " This formatting can be used to feed the command 'open_sesame steering alibava_full_reco'.")
    ld_parser.add_argument('-b','--beam-filename',action='store',dest='beam_file',\
            help="The file name of the beam-type file to obtain the pedestal and calibration"\
            " files associated to it")
    ld_parser.add_argument('-p','--print-available-sensors',dest="available_sensors",\
            action=SensorNamesAction,help='Just print the available'\
            ' sensor names')
    ld_parser.add_argument('-v',dest="verbose",action='store_true',help="Show the list of ignored run numbers"\
            " because of any problem")
    ld_parser.set_defaults(which='list_files',parent_folder=eospath,beam_file=None,ignore_runs=[])
    
    # -- Subparser: steering file creator for the ALIBAVA/TELESCOPE data 
    #               reconstruction
    usage_st  = "Build the needed steering files to run a given step of the marlin"
    usage_st += " framework reconstruction for the ALIBAVA or TELESCOPE data. Note"
    usage_st += " that the steering file is created in the working directory."
    usage_st += " All the available options but the `--print-availble-steps` are"
    usage_st += " directly related with the relevant steering file (see the"
    usage_st += " steering files under `steering_files` directory "

    st_parser = subparsers.add_parser("steering",description=usage_st)
    st_parser.add_argument('step',help="The step name to create the steering file"\
            " [See `--print-available-steps` option] ")
    # XXX: Could it be possible to add all those arguments by looping the template_args,
    #      and converting _ -> -, plus capital leters to "miniscula"
    st_parser.add_argument('--alibava-input-filename',dest="ALIBAVA_INPUT_FILENAME",action='store',\
            help=template_args['ALIBAVA_INPUT_FILENAME'])
    st_parser.add_argument('--alibava-ref-input-filename',dest="ALIBAVA_REF_INPUT_FILENAME",action='store',\
            help=template_args['ALIBAVA_REF_INPUT_FILENAME'])
    st_parser.add_argument('--telescope-input-filename',dest="TELESCOPE_INPUT_FILENAME",action='store',\
            help=template_args['TELESCOPE_INPUT_FILENAME'])
    st_parser.add_argument('--input-filename',dest="INPUT_FILENAMES",action='store',\
            help=template_args['INPUT_FILENAMES'])
    st_parser.add_argument('--active-channels',dest="ACTIVE_CHANNELS",action='store',\
            help=template_args['ACTIVE_CHANNELS'])
    st_parser.add_argument('--enable-automasking',dest="ENABLE_AUTOMASKING",action='store',\
            help=template_args['ENABLE_AUTOMASKING'])
    st_parser.add_argument('--criterium-automasking',dest="CRITERIUM_AUTOMASKING",action='store',\
            help=template_args['CRITERIUM_AUTOMASKING'])
    st_parser.add_argument('--run-number',dest="RUN_NUMBER",action='store',\
            help=template_args['RUN_NUMBER'])
    st_parser.add_argument('--root-filename',dest="ROOT_FILENAME",action='store',\
            help=template_args['ROOT_FILENAME'])
    st_parser.add_argument('--gear-file',dest="GEAR_FILE",action='store',\
            help=template_args['GEAR_FILE'])
    st_parser.add_argument('--output-filename',dest="OUTPUT_FILENAME",action='store',\
            help=template_args['OUTPUT_FILENAME'])
    st_parser.add_argument('--pedestal-output-filename',dest="PEDESTAL_OUTPUT_FILENAME",action='store',\
            help=template_args['PEDESTAL_OUTPUT_FILENAME'])
    st_parser.add_argument('--pedestal-input-filename',dest="PEDESTAL_INPUT_FILENAME",action='store',\
            help=template_args['PEDESTAL_INPUT_FILENAME']+" (Related with --pedestal-ouput-filename)")
    st_parser.add_argument('--histo-xmax',dest="MAXADC",action='store',\
            help=template_args['MAXADC'])
    st_parser.add_argument('--histo-xmin',dest="MINADC",action='store',\
            help=template_args['MINADC'])
    st_parser.add_argument('--histo-nbins',dest="NBINS",action='store',\
            help=template_args['NBINS'])
    st_parser.add_argument('--calibration-output-filename',dest="CALIBRATION_OUTPUT_FILENAME",action='store',\
            help=template_args['CALIBRATION_OUTPUT_FILENAME'])
    st_parser.add_argument('--calibration-input-filename',dest="CALIBRATION_INPUT_FILENAME",action='store',\
            help=template_args['CALIBRATION_INPUT_FILENAME'])
    st_parser.add_argument('--timecut-min',dest="TIMECUT_MIN",action='store',\
            help=template_args['TIMECUT_MIN'])
    st_parser.add_argument('--timecut-max',dest="TIMECUT_MAX",action='store',\
            help=template_args['TIMECUT_MAX'])
    st_parser.add_argument('--cmmdcut-min',dest="CMMDCUT_MIN",action='store',\
            help=template_args['CMMDCUT_MIN'])
    st_parser.add_argument('--cmmdcut-max',dest="CMMDCUT_MAX",action='store',\
            help=template_args['CMMDCUT_MAX'])
    st_parser.add_argument('--max-neighbourg',dest="MAX_NEIGHBOURG",action='store',\
            help=template_args['MAX_NEIGHBOURG'])
    st_parser.add_argument('--clusters-name',dest="CLUSTERS_NAME",action='store',\
            help=template_args['CLUSTERS_NAME'])
    st_parser.add_argument('--remove-clusters',dest="REMOVE_CLUSTERS",action='store_true',\
            help=template_args['REMOVE_CLUSTERS'])
    st_parser.add_argument('--snrcut-seed',dest="SNRCUT_SEED",action='store',\
            help=template_args['SNRCUT_SEED'])
    st_parser.add_argument('--snrcut-neighbour',dest="SNRCUT_NGB",action='store',\
            help=template_args['SNRCUT_NGB'])
    st_parser.add_argument('--signal-polarity',dest="SIGNAL_POLARITY",action='store',\
            help=template_args['SIGNAL_POLARITY'])
    st_parser.add_argument('--sensor-id-starts-at',dest="SENSORID_STARTS",action='store',\
            help=template_args['SENSORID_STARTS'])
    st_parser.add_argument('--prealignment-dump-gear',dest='PREALIGN_DUMP_GEAR',action='store_true',\
            help=template_args['PREALIGN_DUMP_GEAR'])
    st_parser.add_argument('--iteration',dest='ITERATION',action='store',\
            help=template_args['ITERATION'])
    st_parser.add_argument('-p','--print-available-steps',dest="available_steps",\
            action=StepNamesAction,help='Just print the available step names')
    st_parser.set_defaults(which='steering',ignore_runs=[])
    
    # -- Subparser: Marlin post-processing sensor maps
    usage_sm  = "Produce charge and efficiency maps for the "
    usage_sm += "ALiBaVa data acquired sensors, processed using the "
    usage_sm += "Marlin framework. The ALiBaVa data has been merged "
    usage_sm += "with the Telescope data (EUTelescope software) and "
    usage_sm += "has provided a ROOT NTUPLE file with a specific structure "
    usage_sm += "containing both the ALiBaVa sensors data and reconstructed "
    usage_sm += "tracks from the telescope (this is the NTUPLE input for "
    usage_sm += "script)."
    
    sm_parser = subparsers.add_parser("sensor_maps",description=usage_sm)
    sm_parser.add_argument('ntuple',help="The root NTUPLE "\
            " file obtained by the EUTelTreeCreator processor")
    sm_parser.set_defaults(which='sensor_maps')
    
    # -- Subparser: simple file merger when dealing with EORE extra events
    usage_fm  = "Create the steering file to be used to merge a collection "
    usage_fm += "of slcio files containing the 'End of Run Event' (EORE). "
    usage_fm += "The usual method (lcio_merge_files) does not take into "
    usage_fm += "account that last event added by the EUTelescope processors "
    usage_fm += "and it will result in a lost of synchronization. Using the "
    usage_fm += "created steering file here, the EOREs will be removed and "
    usage_fm += "the events merged properly"
    
    fm_parser = subparsers.add_parser("merge_eore",description=usage_fm)
    fm_parser.add_argument('lcio_files',nargs="+",help="The bunch of LCIO files containing"\
            " the EORE 'End of Run Event' added by some EUTelescope processors")
    fm_parser.add_argument('-o','--output',dest='output',action='store',\
            help="The merged file [Default: merged_output.slcio")
    fm_parser.set_defaults(which='merge_eore',output='merged_output.slcio')
    
    # -- Subparser: Post-processing and analysis of the ROOT files created by 
    #               the `fortythieves` executable
    usage_an  = "Post-processing of the ROOT files created by the 'fortythieves' "
    usage_an += "executable, including cluster finding, position and sensor "
    usage_an += "characteristics analysis (eta distributions, charge distributions, ...) "
    
    an_parser = subparsers.add_parser("sensor_analysis",description=usage_an)
    an_parser.add_argument('root_file',help="The `fortythieves` ROOT file, "\
            " containing the pedestal and calibration pre-processing")
    an_parser.add_argument('-c', '--chip-selection',dest='chip_selection',\
            type=int,help="Manually select the chip to process. This option has "
            "precedence over the chip extraction by using the ROOT file name")
    an_parser.add_argument('-t','--time-window', nargs=2,dest='time_window',\
            type=float,help="The TDC time window to cut [Default: 0 100]")
    an_parser.add_argument('--snr-seed',dest='snr_seed',action='store',type=float,\
            help="The Signal to Noise cut to be considered a channel as a seed"\
            " of a cluster [Default: 5]")
    an_parser.add_argument('--snr-neighbour',dest='snr_neighbour',action='store',type=float,\
            help="The Signal to Noise cut to be considered a channel into a cluster"\
            " [Default: 3]")
    an_parser.add_argument('-m',"--masked-channels",nargs="+",dest='masked_channels',type=int,\
            action='store',help="Manual masking channels")
    an_parser.add_argument("--polarity",dest='polarity',type=int,\
            action='store',help="Manual inclusion of the signal polarity (not needed and ignored "\
            " if the file name follows the standard naming convention)")
    an_parser.add_argument("--no-automask",dest='noautomask',action='store_true',\
            help="Change the default behaviour of automasking channels (see --automask-criterium)")
    an_parser.add_argument("--automask-criterium",dest='automask_criterium',action='store',type=float,\
            help="The criterium to used to decide if a channel is noisy or not. "\
            "A noisy channel is defined as |Noise_i - <Noise>| > X sigma [Default: 2.5]")
    an_parser.add_argument("--cross-talk-correction",dest='xtcorrection',action='store',type=int,\
            help="The number of iterations needed for the FIR filter when applying the inductive"\
            " cross-talk correction. A value of 0, no correction is applied (default)")
    an_parser.set_defaults(which='sensor_analysis',snr_seed=5,snr_neighbour=3,\
            time_window=[0.0,100.0],automask_criterium=2.5,xtcorrection=0, polarity=None)

    args = parser.parse_args()
    
    if args.which == 'list_files':
        list_raw_files(args.parent_folder,args.verbose,args.op_format,args.beam_file)
    elif args.which == 'steering':
        valid_args = template_args.keys()
        # Get the list of arguments related with the steering file template
        # which are filled
        the_args = dict(filter(lambda (k,v): k in valid_args and v is not None,args.__dict__.iteritems()))
        # arguments in the template steering files
        create_steering_file(args.step,**the_args)
    elif args.which == "sensor_maps":
        sensor_map_production(args.ntuple)
    elif args.which == 'merge_eore':
        steering_filename=merge_files(args.lcio_files,args.output)
        print "\033[1;34mTo create the merged file, run: \033[1;m\n"\
                " \033[1;29mMarlin {0}\033[1;m".format(steering_filename)
    elif args.which == 'sensor_analysis':
        sensor_analysis(args)
